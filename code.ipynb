{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Molecular Translation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZCiAPuYuPyM5",
        "FnzRU92eJsZj",
        "wXk_OAFeJsZm",
        "UY9X1xNkJsZt",
        "ozoscYJXJsZt",
        "XyL9OMC_JsZ2",
        "8TjlG49SJsZ3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsWH8hWuk_ye"
      },
      "source": [
        "<h1 style=\"text-align: center; font-family: Verdana; font-size: 16px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: small-caps; letter-spacing: 3px; color: #FF1493; background-color: #ffffff;\">Bristol-Myers Squibb – Molecular Translation</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-kwA46xFh0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d22fcb-68df-4ce7-d0db-b99107b4a960"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCiAPuYuPyM5"
      },
      "source": [
        "# Getting Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgQQt76x_5_9"
      },
      "source": [
        " ! pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF0GwWxeALuT"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!mkdir /content/data\n",
        "!cd /content/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTtxq5XrDKbk"
      },
      "source": [
        "!kaggle competitions download -c bms-molecular-translation\n",
        "!kaggle competitions list -s bms-molecular-translation\n",
        "!wget https://www.kaggle.com/c/bms-molecular-translation/data#:~:text=get_app-,Download,-All\n",
        "!kaggle datasets download -d dschettler8845/automl-efficientdet-efficientnetv2\n",
        "!kaggle datasets download -d dschettler8845/bms-csvs-w-extra-metadata\n",
        "!kaggle datasets download -d dschettler8845/bms-test-dataset-192x384\n",
        "!kaggle datasets download -d dschettler8845/bms-train-tfrecords-half-length\n",
        "!kaggle datasets download -d kedarvaidya05/bms-efficientnetv2-tpu-e2e-pipeline-in-3hrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86SVGZyNI84p"
      },
      "source": [
        "!cp /content/bms-train-tfrecords-half-length.zip  /content/drive/MyDrive/Molecular_translation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ASsJHhPN11I"
      },
      "source": [
        "!mkdir /content/drive/MyDrive/Molecular_translation/bms-molecular-translation\n",
        "!mkdir /content/drive/MyDrive/Molecular_translation/automl-efficientdet-efficientnetv2\n",
        "!mkdir /content/drive/MyDrive/Molecular_translation/bms-csvs-w-extra-metadata\n",
        "!mkdir /content/drive/MyDrive/Molecular_translation/bms-test-dataset-192x384\n",
        "!mkdir /content/drive/MyDrive/Molecular_translation/bms-train-tfrecords-half-length\n",
        "!mkdir /content/drive/MyDrive/Molecular_translation/bms-efficientnetv2-tpu-e2e-pipeline-in-3hrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq921kEpNRIj"
      },
      "source": [
        "!unzip /content/drive/MyDrive/Molecular_translation/bms-train-tfrecords-half-length.zip -d /content/drive/MyDrive/Molecular_translation/bms-train-tfrecords-half-length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWdXY51zPY8k"
      },
      "source": [
        "DATA_DIR = \"/content/drive/MyDrive/Molecular_translation\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnzRU92eJsZj"
      },
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4sKn0kgbekd"
      },
      "source": [
        "!pip install --user tensorflow-addons==0.8.3\n",
        "!pip install --user tensorflow==2.2.0-rc3\n",
        "!pip install kaggle_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8rJsqTgcNHW",
        "outputId": "8ab4ad68-1077-4626-e80d-f19dbfc116af"
      },
      "source": [
        "!pip install python-levenshtein"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[?25l\r\r     |██████▌                         | 10 kB 23.3 MB/s eta 0:00:01\r     |█████████████                   | 20 kB 11.8 MB/s eta 0:00:01\r     |███████████████████▌            | 30 kB 9.5 MB/s eta 0:00:01 \r     |██████████████████████████      | 40 kB 8.4 MB/s eta 0:00:01 \r     |████████████████████████████████| 50 kB 2.8 MB/s             \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-levenshtein) (57.4.0)\n",
            "Building wheels for collected packages: python-levenshtein\n",
            "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149867 sha256=4de3c2b904dc996c9618ae07da4758462010ef7d06a72268a00c42ca0968806a\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-levenshtein\n",
            "Installing collected packages: python-levenshtein\n",
            "Successfully installed python-levenshtein-0.12.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-06-15T16:58:51.443597Z",
          "iopub.execute_input": "2021-06-15T16:58:51.444111Z",
          "iopub.status.idle": "2021-06-15T16:59:18.141311Z",
          "shell.execute_reply.started": "2021-06-15T16:58:51.444009Z",
          "shell.execute_reply": "2021-06-15T16:59:18.140248Z"
        },
        "trusted": true,
        "id": "LkzcfmgZJsZj"
      },
      "source": [
        "# Installs\n",
        "print(\"\\n... PIP/APT INSTALLS STARTING ...\\n\")\n",
        "# Pips\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q pydot\n",
        "!pip install -q pydotplus\n",
        "\n",
        "# Apt-get\n",
        "!apt-get install -q graphviz\n",
        "print(\"\\n... PIP/APT INSTALLS COMPLETE ...\\n\")\n",
        "\n",
        "print(\"\\n... IMPORTS STARTING ...\\n\")\n",
        "print(\"\\n\\tVERSION INFORMATION\")\n",
        "# Machine Learning and Data Science Imports\n",
        "import tensorflow as tf; print(f\"\\t\\t– TENSORFLOW VERSION: {tf.__version__}\");\n",
        "import tensorflow_addons as tfa; print(f\"\\t\\t– TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\n",
        "import pandas as pd; pd.options.mode.chained_assignment = None;\n",
        "import numpy as np; print(f\"\\t\\t– NUMPY VERSION: {np.__version__}\");\n",
        "\n",
        "# Library used to easily calculate LD\n",
        "import Levenshtein\n",
        "\n",
        "# Built In Imports\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from glob import glob\n",
        "import warnings\n",
        "import requests\n",
        "import imageio\n",
        "import IPython\n",
        "import urllib\n",
        "import zipfile\n",
        "import pickle\n",
        "import random\n",
        "import shutil\n",
        "import string\n",
        "import math\n",
        "import time\n",
        "import gzip\n",
        "import ast\n",
        "import sys\n",
        "import io\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "\n",
        "# Visualization Imports\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.patches as patches\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm; tqdm.pandas();\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import matplotlib; print(f\"\\t\\t– MATPLOTLIB VERSION: {matplotlib.__version__}\");\n",
        "import plotly\n",
        "import PIL\n",
        "import cv2\n",
        "\n",
        "\n",
        "def seed_it_all(seed=7):\n",
        "    \"\"\" Attempt to be Reproducible \"\"\"\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    \n",
        "print(\"\\n\\n... IMPORTS COMPLETE ...\\n\")\n",
        "    \n",
        "print(\"\\n... SEEDING FOR DETERMINISTIC BEHAVIOUR ...\\n\")\n",
        "seed_it_all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXk_OAFeJsZm"
      },
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOjrMCbJJsZm"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">2.1 ACCELERATOR DETECTION</h3>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T16:59:18.142708Z",
          "iopub.execute_input": "2021-06-15T16:59:18.142987Z",
          "iopub.status.idle": "2021-06-15T16:59:23.423007Z",
          "shell.execute_reply.started": "2021-06-15T16:59:18.142947Z",
          "shell.execute_reply": "2021-06-15T16:59:23.421788Z"
        },
        "trusted": true,
        "id": "i-gcpYQ4JsZn"
      },
      "source": [
        "print(f\"\\n... ACCELERATOR SETUP STARTING ...\\n\")\n",
        "\n",
        "\n",
        "try:\n",
        "    \n",
        "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \n",
        "except ValueError:\n",
        "    TPU = None\n",
        "\n",
        "if TPU:\n",
        "    print(f\"\\n... RUNNING ON TPU - {TPU.master()}...\")\n",
        "    tf.config.experimental_connect_to_cluster(TPU)\n",
        "    tf.tpu.experimental.initialize_tpu_system(TPU)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(TPU)\n",
        "else:\n",
        "    print(f\"\\n... RUNNING ON CPU/GPU ...\")\n",
        "    \n",
        "    strategy = tf.distribute.get_strategy() \n",
        "\n",
        "\n",
        "N_REPLICAS = strategy.num_replicas_in_sync\n",
        "    \n",
        "print(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n",
        "\n",
        "print(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHadFVtOJsZo"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">2.2 COMPETITION DATA ACCESS</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T16:59:23.425341Z",
          "iopub.execute_input": "2021-06-15T16:59:23.425795Z",
          "iopub.status.idle": "2021-06-15T16:59:24.188807Z",
          "shell.execute_reply.started": "2021-06-15T16:59:23.425747Z",
          "shell.execute_reply": "2021-06-15T16:59:24.187523Z"
        },
        "trusted": true,
        "id": "HrutAUwQJsZp"
      },
      "source": [
        "print(\"DATA ACCESS SETUP STARTED\")\n",
        "\n",
        "if TPU:\n",
        "    \n",
        "    DATA_DIR = KaggleDatasets().get_gcs_path('bms-train-tfrecords-half-length')\n",
        "    TEST_DATA_DIR = KaggleDatasets().get_gcs_path('bms-test-dataset-192x384')\n",
        "else:\n",
        "    # Local path to training and validation images\n",
        "    DATA_DIR = f\"{DATA_DIR}/bms-train-tfrecords-half-length\"\n",
        "    TEST_DATA_DIR = f\"{DATA_DIR}/bms-test-dataset-192x384\"\n",
        "    \n",
        "print(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\n",
        "print(f\"... TEST DATA DIRECTORY PATH IS:\\n\\t--> {TEST_DATA_DIR}\")\n",
        "\n",
        "print(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\n",
        "for file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n",
        "\n",
        "print(f\"... IMMEDIATE CONTENTS OF TESTT DATA DIRECTORY IS:\")\n",
        "for file in tf.io.gfile.glob(os.path.join(TEST_DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lvxE7kxJsZp"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">2.3 LEVERAGING MIXED PRECISION</h3>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T16:59:24.190579Z",
          "iopub.execute_input": "2021-06-15T16:59:24.190941Z",
          "iopub.status.idle": "2021-06-15T16:59:24.199821Z",
          "shell.execute_reply.started": "2021-06-15T16:59:24.190908Z",
          "shell.execute_reply": "2021-06-15T16:59:24.198634Z"
        },
        "trusted": true,
        "id": "Tzt8wWTaJsZq"
      },
      "source": [
        "print(f\"\\n... MIXED PRECISION SETUP STARTING ...\\n\")\n",
        "print(\"\\n... SET TF TO OPERATE IN MIXED PRECISION – `bfloat16` – IF ON TPU ...\")\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_bfloat16' if TPU else 'float32')\n",
        "\n",
        "# target data type, bfloat16 when using TPU to improve throughput\n",
        "TARGET_DTYPE = tf.bfloat16 if TPU else tf.float32\n",
        "print(f\"\\t--> THE TARGET DTYPE HAS BEEN SET TO {TARGET_DTYPE} ...\")\n",
        "\n",
        "# The policy specifies two important aspects of a layer: \n",
        "#     1. The dtype the layer's computations are done in\n",
        "#     2. The dtype of a layer's variables. \n",
        "print(f\"\\n... TWO IMPORTANT ASPECTS OF THE GLOBAL MIXED PRECISION POLICY:\")\n",
        "print(f'\\t--> COMPUTE DTYPE  : {tf.keras.mixed_precision.global_policy().compute_dtype}')\n",
        "print(f'\\t--> VARIABLE DTYPE : {tf.keras.mixed_precision.global_policy().variable_dtype}')\n",
        "\n",
        "print(f\"\\n\\n... MIXED PRECISION SETUP COMPLTED ...\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8AqjS8PJsZq"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">2.4 LEVERAGING XLA OPTIMIZATIONS</h3>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T16:59:24.201207Z",
          "iopub.execute_input": "2021-06-15T16:59:24.201527Z",
          "iopub.status.idle": "2021-06-15T16:59:24.21901Z",
          "shell.execute_reply.started": "2021-06-15T16:59:24.201496Z",
          "shell.execute_reply": "2021-06-15T16:59:24.217744Z"
        },
        "trusted": true,
        "id": "30q4t8KaJsZr"
      },
      "source": [
        "print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n",
        "\n",
        "print(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n",
        "# enable XLA optmizations (10% speedup when using @tf.function calls)\n",
        "tf.config.optimizer.set_jit(True)\n",
        "\n",
        "print(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK4tiAxmJsZr"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">2.5 BASIC DATA DEFINITIONS & INITIALIZATIONS</h3>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T16:59:24.220439Z",
          "iopub.execute_input": "2021-06-15T16:59:24.220753Z",
          "iopub.status.idle": "2021-06-15T16:59:24.470651Z",
          "shell.execute_reply.started": "2021-06-15T16:59:24.220723Z",
          "shell.execute_reply": "2021-06-15T16:59:24.46957Z"
        },
        "trusted": true,
        "id": "PQlnf5uCJsZr"
      },
      "source": [
        "print(\"\\n... BASIC DATA SETUP STARTING ...\\n\")\n",
        "\n",
        "# All the possible tokens in our InChI 'language'\n",
        "TOKEN_LIST = [\"<PAD>\", \"InChI=1S/\", \"<END>\", \"/c\", \"/h\", \"/m\", \"/t\", \"/b\", \"/s\", \"/i\"] +\\\n",
        "             ['Si', 'Br', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', 'C', 'H', 'B', ] +\\\n",
        "             [str(i) for i in range(167,-1,-1)] +\\\n",
        "             [\"\\+\", \"\\(\", \"\\)\", \"\\-\", \",\", \"D\", \"T\"]\n",
        "print(f\"\\n... TOKEN LIST:\")\n",
        "for i, tok in enumerate(TOKEN_LIST): print(f\"\\t--> INTEGER-IDX = {i:<3}  –––  STRING = {tok}\")\n",
        "\n",
        "START_TOKEN = tf.constant(TOKEN_LIST.index(\"InChI=1S/\"), dtype=tf.uint8)\n",
        "END_TOKEN = tf.constant(TOKEN_LIST.index(\"<END>\"), dtype=tf.uint8)\n",
        "PAD_TOKEN = tf.constant(TOKEN_LIST.index(\"<PAD>\"), dtype=tf.uint8)\n",
        "\n",
        "PREFIX_ORDERING = \"chbtmsihtm\"\n",
        "print(f\"\\n... PREFIX ORDERING IS {PREFIX_ORDERING} ...\")\n",
        "\n",
        "# Paths to Respective Image Directories\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, \"train_records\")\n",
        "VAL_DIR = os.path.join(DATA_DIR, \"val_records\")\n",
        "TEST_DIR = os.path.join(TEST_DATA_DIR, \"test_records\")\n",
        "\n",
        "# Get the Full Paths to The Individual TFRecord Files\n",
        "TRAIN_TFREC_PATHS = sorted(\n",
        "    tf.io.gfile.glob(os.path.join(TRAIN_DIR, \"*.tfrec\")), \n",
        "    key=lambda x: int(x.rsplit(\"_\", 2)[1]))\n",
        "VAL_TFREC_PATHS = sorted(\n",
        "    tf.io.gfile.glob(os.path.join(VAL_DIR, \"*.tfrec\")), \n",
        "    key=lambda x: int(x.rsplit(\"_\", 2)[1]))\n",
        "TEST_TFREC_PATHS = sorted(\n",
        "    tf.io.gfile.glob(os.path.join(TEST_DIR, \"*.tfrec\")), \n",
        "    key=lambda x: int(x.rsplit(\"_\", 2)[1]))\n",
        "\n",
        "print(f\"\\n... TFRECORD INFORMATION:\")\n",
        "for SPLIT, TFREC_PATHS in zip([\"TRAIN\", \"VAL\", \"TEST\"], [TRAIN_TFREC_PATHS, VAL_TFREC_PATHS, TEST_TFREC_PATHS]):\n",
        "    print(f\"\\t--> {len(TFREC_PATHS):<3} {SPLIT:<5} TFRECORDS\")\n",
        "\n",
        "# Paths to relevant CSV files containing training and submission information\n",
        "TRAIN_CSV_PATH = os.path.join(DATA_DIR, \"bms-csvs-w-extra-metadata\", \"train_labels_w_extra.csv\")\n",
        "SS_CSV_PATH    = os.path.join(DATA_DIR, \"bms-csvs-w-extra-metadata\", \"sample_submission_w_extra.csv\")\n",
        "print(f\"\\n... PATHS TO CSVS:\")\n",
        "print(f\"\\t--> TRAIN CSV: {TRAIN_CSV_PATH}\")\n",
        "print(f\"\\t--> SS CSV   : {SS_CSV_PATH}\")\n",
        "\n",
        "# When debug is true we use a smaller batch size and smaller model\n",
        "DEBUG=False\n",
        "\n",
        "print(\"\\n\\n... BASIC DATA SETUP COMPLETED ...\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ROQYjXQJsZs"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">2.6 INITIAL DATAFRAME INSTANTIATION</h3>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T16:59:24.47202Z",
          "iopub.execute_input": "2021-06-15T16:59:24.472323Z",
          "iopub.status.idle": "2021-06-15T16:59:44.312001Z",
          "shell.execute_reply.started": "2021-06-15T16:59:24.472291Z",
          "shell.execute_reply": "2021-06-15T16:59:44.310853Z"
        },
        "trusted": true,
        "id": "4fyUf5FcJsZs"
      },
      "source": [
        "print(\"\\n... INITIAL DATAFRAME INSTANTIATION STARTING ...\\n\")\n",
        "\n",
        "# Load the train and submission dataframes\n",
        "train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
        "ss_df    = pd.read_csv(SS_CSV_PATH)\n",
        "\n",
        "# --- Distribution Information ---\n",
        "N_EX    = len(train_df)\n",
        "N_TEST  = len(ss_df)\n",
        "N_VAL   = 80_000 # Fixed from dataset creation information\n",
        "N_TRAIN = N_EX-N_VAL\n",
        "\n",
        "# --- Batching Information ---\n",
        "DEBUG=False\n",
        "BATCH_SIZE_DEBUG   = 2\n",
        "REPLICA_BATCH_SIZE = 128 # Could probably be 128\n",
        "\n",
        "if DEBUG:\n",
        "    REPLICA_BATCH_SIZE = BATCH_SIZE_DEBUG\n",
        "OVERALL_BATCH_SIZE = REPLICA_BATCH_SIZE*N_REPLICAS\n",
        "\n",
        "\n",
        "# --- Input Image Information ---\n",
        "IMG_SHAPE = (192,384,3)\n",
        "\n",
        "# --- Autocalculate Training/Validation/Testing Information ---\n",
        "TRAIN_STEPS = N_TRAIN  // OVERALL_BATCH_SIZE\n",
        "VAL_STEPS   = N_VAL    // OVERALL_BATCH_SIZE\n",
        "TEST_STEPS  = int(np.ceil(N_TEST/OVERALL_BATCH_SIZE))\n",
        "\n",
        "# This is for padding our test dataset so we only have whole batches\n",
        "REQUIRED_DATASET_PAD = OVERALL_BATCH_SIZE-N_TEST%OVERALL_BATCH_SIZE\n",
        "\n",
        "# --- Modelling Information ---\n",
        "ATTN_EMB_DIM  = 192\n",
        "N_RNN_UNITS   = 512\n",
        "\n",
        "print(f\"\\n... # OF TRAIN+VAL EXAMPLES  : {N_EX:<7} ...\")\n",
        "print(f\"... # OF TRAIN EXAMPLES      : {N_TRAIN:<7} ...\")\n",
        "print(f\"... # OF VALIDATION EXAMPLES : {N_VAL:<7} ...\")\n",
        "print(f\"... # OF TEST EXAMPLES       : {N_TEST:<7} ...\\n\")\n",
        "\n",
        "print(f\"\\n... REPLICA BATCH SIZE    : {REPLICA_BATCH_SIZE} ...\")\n",
        "print(f\"... OVERALL BATCH SIZE    : {OVERALL_BATCH_SIZE} ...\\n\")\n",
        "\n",
        "print(f\"\\n... IMAGE SHAPE           : {IMG_SHAPE} ...\\n\")\n",
        "\n",
        "print(f\"\\n... TRAIN STEPS PER EPOCH : {TRAIN_STEPS:<5} ...\")\n",
        "print(f\"... VAL STEPS PER EPOCH   : {VAL_STEPS:<5} ...\")\n",
        "print(f\"... TEST STEPS PER EPOCH  : {TEST_STEPS:<5} ...\\n\")\n",
        "\n",
        "print(\"\\n... TRAIN DATAFRAME ...\\n\")\n",
        "display(train_df.head(3))\n",
        "\n",
        "print(\"\\n... SUBMISSION DATAFRAME ...\\n\")\n",
        "display(ss_df.head(3))\n",
        "\n",
        "print(\"\\n... INITIAL DATAFRAME INSTANTIATION COMPLETED...\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTY80sQsJsZs"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">2.7 USER INPUT VARIABLES</h3>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T16:59:44.316145Z",
          "iopub.execute_input": "2021-06-15T16:59:44.316439Z",
          "iopub.status.idle": "2021-06-15T16:59:44.324227Z",
          "shell.execute_reply.started": "2021-06-15T16:59:44.316409Z",
          "shell.execute_reply": "2021-06-15T16:59:44.323097Z"
        },
        "trusted": true,
        "id": "6LhzxA3TJsZs"
      },
      "source": [
        "print(\"\\n... SPECIAL VARIABLE SETUP STARTING ...\\n\")\n",
        "\n",
        "\n",
        "# Whether to start training using previously checkpointed model\n",
        "LOAD_MODEL        = False\n",
        "ENCODER_CKPT_PATH = \"\"\n",
        "TRANSFORMER_CKPT_PATH = \"\"\n",
        "\n",
        "if LOAD_MODEL:\n",
        "    if TRANSFORMER_CKPT_PATH != \"\":\n",
        "        print(f\"... TRANSFORMER MODEL TRAINING WILL RESUME FROM PREVIOUS CHECKPOINT:\\n\\t-->{TRANSFORMER_CKPT_PATH}\\n\")\n",
        "    elif ENCODER_CKPT_PATH != \"\":\n",
        "        print(f\"\\n... ENCODER MODEL TRAINING WILL RESUME FROM PREVIOUS CHECKPOINT:\\n\\t-->{ENCODER_CKPT_PATH}\\n\")    \n",
        "    else:\n",
        "        print(f\"\\n... MODEL TRAINING WILL START FROM SCRATCH ...\\n\")\n",
        "else:\n",
        "    print(f\"\\n... MODEL TRAINING WILL START FROM SCRATCH ...\\n\")\n",
        "\n",
        "    \n",
        "print(\"\\n... SPECIAL VARIABLE SETUP COMPLETED ...\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY9X1xNkJsZt"
      },
      "source": [
        "#HELPER FUNCTION & CLASSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYZxVATCJsZt"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">3.1 GENERAL HELPER FUNCTIONS</h3>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T16:59:44.326488Z",
          "iopub.execute_input": "2021-06-15T16:59:44.326794Z",
          "iopub.status.idle": "2021-06-15T16:59:44.345487Z",
          "shell.execute_reply.started": "2021-06-15T16:59:44.326757Z",
          "shell.execute_reply": "2021-06-15T16:59:44.344273Z"
        },
        "trusted": true,
        "id": "6c8n4tpRJsZt"
      },
      "source": [
        "def flatten_l_o_l(nested_list):\n",
        "    \"\"\" Flatten a list of lists \"\"\"\n",
        "    return [item for sublist in nested_list for item in sublist]\n",
        "\n",
        "\n",
        "def tf_load_image(path, img_size=(192,384,3), invert=False):\n",
        "    \n",
        "    img = decode_img(tf.io.read_file(path), img_size, n_channels=3, invert=invert)        \n",
        "    return img\n",
        "    \n",
        "    \n",
        "def decode_image(image_data, resize_to=(192,384,3)):\n",
        "    \n",
        "    image = tf.image.decode_png(image_data, channels=3)\n",
        "    image = tf.reshape(image, resize_to)\n",
        "    return tf.cast(image, TARGET_DTYPE)\n",
        "    \n",
        "    \n",
        "# sparse tensors are required to compute the Levenshtein distance\n",
        "def dense_to_sparse(dense):\n",
        "    \n",
        "    indices = tf.where(tf.ones_like(dense))\n",
        "    values = tf.reshape(dense, (MAX_LEN*OVERALL_BATCH_SIZE,))\n",
        "    sparse = tf.SparseTensor(indices, values, dense.shape)\n",
        "    return sparse\n",
        "\n",
        "def get_levenshtein_distance(preds, lbls):\n",
        "    \n",
        "    preds = tf.where(tf.not_equal(lbls, END_TOKEN) & tf.not_equal(lbls, PAD_TOKEN), preds, 0)\n",
        "    lbls = tf.where(tf.not_equal(lbls, END_TOKEN), lbls, 0)\n",
        "\n",
        "    preds_sparse = dense_to_sparse(preds)\n",
        "    lbls_sparse = dense_to_sparse(lbls)\n",
        "\n",
        "    batch_distance = tf.edit_distance(preds_sparse, lbls_sparse, normalize=False)\n",
        "    mean_distance = tf.math.reduce_mean(batch_distance)\n",
        "    \n",
        "    return mean_distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozoscYJXJsZt"
      },
      "source": [
        "# PREPARE THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T16:59:44.347283Z",
          "iopub.execute_input": "2021-06-15T16:59:44.347872Z",
          "iopub.status.idle": "2021-06-15T16:59:44.37588Z",
          "shell.execute_reply.started": "2021-06-15T16:59:44.347822Z",
          "shell.execute_reply": "2021-06-15T16:59:44.374853Z"
        },
        "trusted": true,
        "id": "UNrDLEiYJsZu"
      },
      "source": [
        "print(\"\\n\\n... STARTING PREPARING VARIABLES FOR DATASET ...\\n\")\n",
        "\n",
        "tok_2_int = {c.strip(\"\\\\\"):i for i,c in enumerate(TOKEN_LIST)}\n",
        "int_2_tok = {v:k for k,v in tok_2_int.items()}\n",
        "\n",
        "# Max Length Was Determined Previously Using... \n",
        "#     >>> MAX_LEN = train_df.InChI.progress_apply(lambda x: len(re.findall(\"|\".join(TOKEN_LIST), x))).max()+1\n",
        "MAX_LEN = ((train_df.inchi_token_len.max()+1)//2) # //2 yields 138... which is half of max length (speeds up training)\n",
        "VOCAB_LEN = len(int_2_tok)\n",
        "\n",
        "print(f\"\\t--> TOKEN TO INTEGER MAP     : {tok_2_int}\")\n",
        "print(f\"\\t--> INTEGER TO TOKEN MAP     : {int_2_tok}\")\n",
        "print(f\"\\t--> MAX # OF TOKENS IN INCHI : {MAX_LEN}\")\n",
        "print(f\"\\t--> LENGTH OF VOCAB          : {VOCAB_LEN}\")\n",
        "\n",
        "print(f\"\\n\\n\\t--> CONVERTED INCHI STRINGS  :\")\n",
        "for i, row in train_df.iloc[:N_VAL].sample(3).iterrows():\n",
        "    print(f\"\\n\\t\\t--> EXAMPLE #{i} FROM THE VALIDATION DATASET\")\n",
        "    print(\"\\t\\t\\t--> RAW INCHI : \", row[\"InChI\"])\n",
        "\n",
        "print(\"\\n\\n... PREPARING VARIABLES FOR DATASET COMPLETED ...\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T16:59:44.46062Z",
          "iopub.execute_input": "2021-06-15T16:59:44.46094Z",
          "iopub.status.idle": "2021-06-15T16:59:44.471692Z",
          "shell.execute_reply.started": "2021-06-15T16:59:44.460908Z",
          "shell.execute_reply": "2021-06-15T16:59:44.470316Z"
        },
        "trusted": true,
        "id": "mS0goBoRJsZu"
      },
      "source": [
        "def decode(serialized_example, is_test=False, tokenized_inchi=True):\n",
        "    \n",
        "    feature_dict = {\n",
        "        'image': tf.io.FixedLenFeature(shape=[], dtype=tf.string, default_value=''),\n",
        "    }\n",
        "    \n",
        "    if not is_test:\n",
        "        if tokenized_inchi:\n",
        "            feature_dict[\"inchi\"] = tf.io.FixedLenFeature(shape=[MAX_LEN], dtype=tf.int64, default_value=[0]*MAX_LEN)\n",
        "        else:\n",
        "            feature_dict[\"inchi\"] = tf.io.FixedLenFeature(shape=[], dtype=tf.string, default_value='')\n",
        "    else:\n",
        "        feature_dict[\"image_id\"] = tf.io.FixedLenFeature(shape=[], dtype=tf.string, default_value='')\n",
        "    \n",
        "    # Define a parser\n",
        "    features = tf.io.parse_single_example(serialized_example, features=feature_dict)\n",
        "    \n",
        "    # Decode the tf.string\n",
        "    image = decode_image(features['image'], resize_to=IMG_SHAPE)\n",
        "    \n",
        "    # Figure out the correct information to return\n",
        "    if is_test:\n",
        "        image_id = features[\"image_id\"] \n",
        "        return image, image_id\n",
        "    else:\n",
        "        if tokenized_inchi:\n",
        "            target = tf.cast(features[\"inchi\"], tf.uint8)\n",
        "        else:\n",
        "            target = features[\"inchi\"]\n",
        "        return image, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T16:59:44.473431Z",
          "iopub.execute_input": "2021-06-15T16:59:44.473798Z",
          "iopub.status.idle": "2021-06-15T16:59:45.309458Z",
          "shell.execute_reply.started": "2021-06-15T16:59:44.473764Z",
          "shell.execute_reply": "2021-06-15T16:59:45.308264Z"
        },
        "trusted": true,
        "id": "XJc9Uy6aJsZv"
      },
      "source": [
        "print(\"\\n... DECODING RAW TFRECORD DATASETS STARTING ...\\n\")\n",
        "\n",
        "# Decode the tfrecords completely –– decode is our `_parse_function` (from recipe above)\n",
        "train_ds = raw_train_ds.map(lambda x: decode(x, is_test=False))\n",
        "val_ds = raw_val_ds.map(lambda x: decode(x, is_test=False))\n",
        "test_ds = raw_test_ds.map(lambda x: decode(x, is_test=True))\n",
        "\n",
        "print(f\"\\n... THE DECODED TF.DATA.TFRECORDDATASET OBJECT:\" \\\n",
        "      f\"\\n\\t--> ((image), (image_id - optional), (inchi))\" \\\n",
        "      f\"\\n\\t--> {train_ds}\\n\")\n",
        "\n",
        "print(\"\\n... 2 EXAMPLES OF IMAGES AND LABELS AFTER DECODING ...\")\n",
        "for i, (img, inchi) in enumerate(train_ds.take(2)):\n",
        "    print(f\"\\nIMAGE SHAPE : {img.shape}\")\n",
        "    print(f\"IMAGE INCHI : {[int_2_tok[x] for x in inchi.numpy()]}\\n\")\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(img.numpy().astype(np.int64), cmap=\"gray\")\n",
        "    plt.title(f\"{''.join([int_2_tok[x] for x in inchi.numpy() if x!=0][:50])} ... [truncated]\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n... DECODING RAW TFRECORD DATASETS COMPLETED ...\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozT7WgXzJsZv"
      },
      "source": [
        "#MODEL PREPARATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZNez2wjJsZw"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.1 UNDERSTANDING THE MODELS - ViT</h3>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylhaUj0hJsZw"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 16px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.1.1 ViT - Implement Patch Creation as a Layer</h3>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T16:59:48.327619Z",
          "iopub.execute_input": "2021-06-15T16:59:48.328149Z",
          "iopub.status.idle": "2021-06-15T17:00:05.292513Z",
          "shell.execute_reply.started": "2021-06-15T16:59:48.328093Z",
          "shell.execute_reply": "2021-06-15T17:00:05.291311Z"
        },
        "trusted": true,
        "id": "uj4znZ9JJsZw"
      },
      "source": [
        "class PatchCreator(tf.keras.layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(PatchCreator, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    \n",
        "\n",
        "demo_img, demo_lbl = next(iter(train_ds.unbatch().batch(1)))\n",
        "demo_patch_size=16\n",
        "\n",
        "\n",
        "with tf.device('/CPU:0'):\n",
        "    patch_creator = PatchCreator(demo_patch_size)\n",
        "    patches = patch_creator(demo_img)\n",
        "\n",
        "print(f\"Image size: {IMG_SHAPE}\")\n",
        "print(f\"Patch size: {(demo_patch_size, demo_patch_size)}\")\n",
        "print(f\"Patches shape: {patches.shape}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "\n",
        "print(\"\\n\\n... ORIGINAL IMAGE ...\\n\")\n",
        "plt.figure(figsize=(18,9))\n",
        "plt.imshow(demo_img[0].numpy().astype(\"float32\")/255.)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n... IMAGE PATCHES ...\\n\")\n",
        "plt.figure(figsize=(18,9))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(int(np.ceil(IMG_SHAPE[0]/demo_patch_size)), int(np.ceil(IMG_SHAPE[1]/demo_patch_size)), i + 1)\n",
        "    patch_img = tf.reshape(patch, (demo_patch_size, demo_patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"float32\")/255.)\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(wspace=0.03, hspace=0.06)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:00:05.294121Z",
          "iopub.execute_input": "2021-06-15T17:00:05.294531Z",
          "iopub.status.idle": "2021-06-15T17:00:05.30589Z",
          "shell.execute_reply.started": "2021-06-15T17:00:05.294487Z",
          "shell.execute_reply": "2021-06-15T17:00:05.304853Z"
        },
        "trusted": true,
        "id": "wFTgrcFbJsZw"
      },
      "source": [
        "tf.range(start=0, limit=288, delta=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:05:04.30272Z",
          "iopub.execute_input": "2021-06-15T17:05:04.303145Z",
          "iopub.status.idle": "2021-06-15T17:05:33.949038Z",
          "shell.execute_reply.started": "2021-06-15T17:05:04.303113Z",
          "shell.execute_reply": "2021-06-15T17:05:33.94794Z"
        },
        "trusted": true,
        "id": "U-qwQlQCJsZw"
      },
      "source": [
        "class PatchEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        \n",
        "        self.num_patches = num_patches\n",
        "        self.projection_dim = projection_dim\n",
        "        \n",
        "        self.dense_projection = tf.keras.layers.Dense(units=self.projection_dim)\n",
        "        self.positions = tf.reshape(tf.range(start=0, limit=self.num_patches, delta=1), (self.num_patches,))\n",
        "        self.position_embedding = tf.keras.layers.Embedding(input_dim=self.num_patches, \n",
        "                                                            output_dim=self.projection_dim)\n",
        "\n",
        "    def call(self, patch):\n",
        "        encoded = self.dense_projection(patch) + self.position_embedding(self.positions)\n",
        "        return encoded\n",
        "    \n",
        "demo_projection_dim = 128\n",
        "\n",
        "with tf.device('/CPU:0'):\n",
        "    patch_encoder = PatchEncoder(num_patches=patches.shape[1], projection_dim=demo_projection_dim)\n",
        "    encoded_patches = patch_encoder(patches)\n",
        "\n",
        "print(f\"Number of Patches: {patches.shape[1]}\")    \n",
        "print(f\"Patch size: {(demo_patch_size, demo_patch_size)}\")\n",
        "print(f\"Encoded patches shape: {encoded_patches.shape}\")\n",
        "\n",
        "print(\"\\n\\n... ORIGINAL IMAGE ...\\n\")\n",
        "plt.figure(figsize=(18,9))\n",
        "plt.imshow(demo_img[0].numpy().astype(\"float32\")/255.)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n\\n... IMAGE PATCHES ...\\n\")\n",
        "plt.figure(figsize=(18,9))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    w,h = int(np.ceil(IMG_SHAPE[0]/demo_patch_size)), int(np.ceil(IMG_SHAPE[1]/demo_patch_size))\n",
        "    ax = plt.subplot(w,h,i+1)\n",
        "    patch_img = tf.reshape(patch, (demo_patch_size, demo_patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"float32\")/255.)\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(wspace=0.03, hspace=0.06)\n",
        "plt.show()\n",
        "\n",
        "with tf.device('/CPU:0'):\n",
        "    enc_patches_rescaled = patch_creator(tf.expand_dims(tf.image.resize(tf.reshape(tf.reduce_mean(encoded_patches[0], axis=-1), (w, h, 1)), (IMG_SHAPE[0], IMG_SHAPE[1])), axis=0))\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n... IMAGE EMBEDDING AS PATCH VISUALIZATION ...\\n\")\n",
        "plt.figure(figsize=(18,9))\n",
        "for i, enc_patch in enumerate(enc_patches_rescaled[0]):\n",
        "    ax = plt.subplot(w,h,i+1)\n",
        "    enc_patch_img = tf.reshape(enc_patch, (demo_patch_size, demo_patch_size))\n",
        "    plt.imshow(enc_patch_img.numpy().astype(\"float32\")/255., cmap=\"jet\")\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(wspace=0.03, hspace=0.06)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bs3pRTUJsZw"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 16px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.1.3 Build the ViT Model</h3>\n",
        "\n",
        "---\n",
        "\n",
        "The ViT model consists of multiple Transformer blocks, which use the layers.MultiHeadAttention layer as a self-attention mechanism applied to the sequence of patches. The Transformer blocks produce a **[batch_size, num_patches, projection_dim]** tensor, which is processed via an classifier head with softmax to produce the final class probabilities output.\n",
        "\n",
        "Unlike the technique described in the paper, which prepends a learnable embedding to the sequence of encoded patches to serve as the image representation, all the outputs of the final Transformer block are reshaped with **`tf.keras.layers.Flatten()`** and used as the image representation input to the classifier head. Note that the **`tf.keras.layers.GlobalAveragePooling1D`** layer could also be used instead to aggregate the outputs of the Transformer block, especially when the number of patches and the projection dimensions are large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:05:33.950585Z",
          "iopub.execute_input": "2021-06-15T17:05:33.95087Z",
          "iopub.status.idle": "2021-06-15T17:06:19.211482Z",
          "shell.execute_reply.started": "2021-06-15T17:05:33.950842Z",
          "shell.execute_reply": "2021-06-15T17:06:19.20857Z"
        },
        "trusted": true,
        "id": "ZKlN71dcJsZx"
      },
      "source": [
        "class ViTEncoder(tf.keras.Model):\n",
        "    def __init__(self, patch_size=16, projection_dim=256, n_transformer_layers=8, n_heads=4, dropout=0.1, img_shape=IMG_SHAPE):\n",
        "\n",
        "        super(ViTEncoder, self).__init__()\n",
        "        \n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = tf.cast(tf.round((img_shape[0]/self.patch_size)*(img_shape[1]/self.patch_size)), tf.int32)\n",
        "        self.img_shape = img_shape\n",
        "        self.projection_dim = projection_dim\n",
        "        self.n_transformer_layers = n_transformer_layers\n",
        "        self.mlp_intermediate_units = [self.projection_dim*2, self.projection_dim,]\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.patch_creator = PatchCreator(self.patch_size)\n",
        "        self.patch_encoder = PatchEncoder(self.n_patches, self.projection_dim)\n",
        "        self.ln_1_layer = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ln_2_layer = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.add_1_layer = tf.keras.layers.Add()\n",
        "        self.add_2_layer = tf.keras.layers.Add()\n",
        "        self.mha_layer = tf.keras.layers.MultiHeadAttention(num_heads=self.n_heads, key_dim=self.projection_dim, dropout=self.dropout)\n",
        "        self.intermediate_mlp_dense_layers   = [tf.keras.layers.Dense(transformer_units, activation=tf.nn.gelu) for transformer_units in self.mlp_intermediate_units]\n",
        "        self.intermediate_mlp_dropout_layers = [tf.keras.layers.Dropout(self.dropout) for transformer_units in self.mlp_intermediate_units]                \n",
        "        \n",
        "    def call(self, x, training):\n",
        "\n",
        "        patches = self.patch_creator(x, training=training)\n",
        "        encoded = self.patch_encoder(patches, training=training)\n",
        "        \n",
        "        for _ in range(self.n_transformer_layers):\n",
        "            \n",
        "            x1 = self.ln_1_layer(encoded, training=training)\n",
        "            \n",
        "            \n",
        "            attention_output = self.mha_layer(x1, x1, training=training)\n",
        "            \n",
        "            \n",
        "            x2 = self.add_1_layer([attention_output, encoded], training=training)\n",
        "            \n",
        "            \n",
        "            x3 = self.ln_2_layer(x2, training=training)\n",
        "            \n",
        "            \n",
        "            for i in range(len(self.mlp_intermediate_units)):\n",
        "                x3 = self.intermediate_mlp_dense_layers[i](x3, training=training)\n",
        "                x3 = self.intermediate_mlp_dropout_layers[i](x3, training=training)\n",
        "            \n",
        "            \n",
        "            encoded = self.add_2_layer([x3, x2], training=training)\n",
        "            \n",
        "        return encoded\n",
        "\n",
        "with tf.device('/CPU:0'):\n",
        "    ViT = ViTEncoder(patch_size=demo_patch_size, projection_dim=demo_projection_dim)\n",
        "    demo_encoder_output= ViT(tf.ones((1, *IMG_SHAPE)))\n",
        "    IMG_SEQ_LEN, IMG_EMB_DEPTH = demo_encoder_output.shape[1], demo_encoder_output.shape[2]\n",
        "    \n",
        "print(f\"Encoder Output Shape: {demo_encoder_output.shape}\")    \n",
        "print(f\"Output 'Sequence' Length: {IMG_SEQ_LEN}\")\n",
        "print(f\"Output 'Sequence' Feature Depth: {IMG_EMB_DEPTH}\")\n",
        "\n",
        "\n",
        "print(\"\\n\\n... ORIGINAL IMAGE ...\\n\")\n",
        "plt.figure(figsize=(18,9))\n",
        "plt.imshow(demo_img[0].numpy().astype(\"float32\")/255.)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n\\n... IMAGE PATCHES ...\\n\")\n",
        "plt.figure(figsize=(18,9))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    w,h = int(np.ceil(IMG_SHAPE[0]/demo_patch_size)), int(np.ceil(IMG_SHAPE[1]/demo_patch_size))\n",
        "    ax = plt.subplot(w,h,i+1)\n",
        "    patch_img = tf.reshape(patch, (demo_patch_size, demo_patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"float32\")/255.)\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(wspace=0.03, hspace=0.06)\n",
        "plt.show()\n",
        "\n",
        "with tf.device('/CPU:0'):\n",
        "    enc_patches_rescaled = patch_creator(tf.expand_dims(tf.image.resize(tf.reshape(tf.reduce_mean(encoded_patches[0], axis=-1), (w, h, 1)), (IMG_SHAPE[0], IMG_SHAPE[1])), axis=0))\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n... IMAGE EMBEDDING AS PATCH VISUALIZATION ...\\n\")\n",
        "plt.figure(figsize=(18,9))\n",
        "for i, enc_patch in enumerate(enc_patches_rescaled[0]):\n",
        "    ax = plt.subplot(w,h,i+1)\n",
        "    enc_patch_img = tf.reshape(enc_patch, (demo_patch_size, demo_patch_size))\n",
        "    plt.imshow(enc_patch_img.numpy().astype(\"float32\")/255., cmap=\"jet\")\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(wspace=0.03, hspace=0.06)\n",
        "plt.show()\n",
        "\n",
        "with tf.device('/CPU:0'):\n",
        "    tmp_img = tf.image.resize(tf.reshape(tf.reduce_mean(demo_encoder_output[0], axis=-1), (w, h, 1)), (IMG_SHAPE[0], IMG_SHAPE[1]))\n",
        "    tmp_img = tf.cast(255*((tmp_img-tf.math.reduce_min(tmp_img))/(tf.math.reduce_max(tmp_img)-tf.math.reduce_min(tmp_img))), dtype=tf.uint8)\n",
        "    enc_output_patches_rescaled = patch_creator(tf.expand_dims(tmp_img, axis=0))\n",
        "\n",
        "print(\"\\n\\n\\n... ENCODER OUTPUT AS PATCH VISUALIZATION ...\\n\")\n",
        "plt.figure(figsize=(18,9))\n",
        "for i, enc_out_patch in enumerate(enc_output_patches_rescaled[0]):\n",
        "    ax = plt.subplot(w,h,i+1)\n",
        "    enc_out_patch_img = tf.reshape(enc_out_patch, (demo_patch_size, demo_patch_size))\n",
        "    plt.imshow(enc_out_patch_img.numpy().astype(\"float32\")/255., cmap=\"jet\")\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(wspace=0.03, hspace=0.06)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n\\n\\n\\tSUMMARY\\n\")\n",
        "ViT.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARo53aMrJsZx"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 16px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.2.0 TRANSFORMER - HYPERPARAMETERS</h3>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:19.213551Z",
          "iopub.execute_input": "2021-06-15T17:06:19.213914Z",
          "iopub.status.idle": "2021-06-15T17:06:19.222439Z",
          "shell.execute_reply.started": "2021-06-15T17:06:19.213878Z",
          "shell.execute_reply": "2021-06-15T17:06:19.22144Z"
        },
        "trusted": true,
        "id": "cWS2QIaJJsZx"
      },
      "source": [
        "D_MODEL = IMG_EMB_DEPTH\n",
        "N_PE_POS = 72\n",
        "D_FF = 1024\n",
        "\n",
        "print(f\"\\n... THE INPUT 'SEQUENCE LENGTH'                  IS {IMG_SEQ_LEN}  (output of image encoder - shape flattened) ...\")\n",
        "print(f\"... THE INPUT 'EMBEDDING DEPTH'                  IS {IMG_EMB_DEPTH}  (output of image encoder - # of channels) ...\")\n",
        "print(f\"... THE NUMBER OF POSITIONAL ENCODING POSITIONS  IS {N_PE_POS}   (arbitray) ...\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:19.224236Z",
          "iopub.execute_input": "2021-06-15T17:06:19.224607Z",
          "iopub.status.idle": "2021-06-15T17:06:19.579515Z",
          "shell.execute_reply.started": "2021-06-15T17:06:19.224567Z",
          "shell.execute_reply": "2021-06-15T17:06:19.578453Z"
        },
        "trusted": true,
        "id": "VaxFFL7fJsZx"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = tf.constant(1, TARGET_DTYPE) / tf.math.pow(tf.constant(10000, TARGET_DTYPE), (tf.constant(2, dtype=TARGET_DTYPE) * tf.cast((i//2), TARGET_DTYPE))/d_model)\n",
        "    return pos * angle_rates\n",
        "\n",
        "def do_interleave(arr_a, arr_b):\n",
        "    a_arr_tf_column = tf.range(arr_a.shape[1])*2 # [0 2 4 ...]\n",
        "    b_arr_tf_column = tf.range(arr_b.shape[1])*2+1 # [1 3 5 ...]\n",
        "    column_indices = tf.argsort(tf.concat([a_arr_tf_column,b_arr_tf_column],axis=-1))\n",
        "    column, row = tf.meshgrid(column_indices,tf.range(arr_a.shape[0]))\n",
        "    combine_indices = tf.stack([row,column],axis=-1)\n",
        "    combine_value = tf.concat([arr_a,arr_b],axis=1)\n",
        "    return tf.gather_nd(combine_value,combine_indices)\n",
        "\n",
        "def positional_encoding_1d(position, d_model):\n",
        "    angle_rads = get_angles(tf.cast(tf.range(position)[:, tf.newaxis], TARGET_DTYPE),\n",
        "                            tf.cast(tf.range(d_model)[tf.newaxis, :], TARGET_DTYPE),\n",
        "                            d_model)\n",
        "    \n",
        "    sin_angle_rads = tf.math.sin(angle_rads[:, ::2])\n",
        "    cos_angle_rads = tf.math.cos(angle_rads[:, 1::2])\n",
        "    angle_rads = do_interleave(sin_angle_rads, cos_angle_rads)\n",
        "    pos_encoding = angle_rads[tf.newaxis, ...]\n",
        "    return pos_encoding\n",
        "\n",
        "def np_positional_encoding_2d(row,col,d_model):\n",
        "    \n",
        "    assert d_model % 2 == 0\n",
        "    row_pos = np.repeat(np.arange(row),col)[:,np.newaxis]\n",
        "    col_pos = np.repeat(np.expand_dims(np.arange(col),0),row,axis=0).reshape(-1,1)\n",
        "\n",
        "    angle_rads_row = get_angles(row_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2).numpy()\n",
        "    angle_rads_col = get_angles(col_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2).numpy()\n",
        "    \n",
        "    angle_rads_row[:, 0::2] = np.sin(angle_rads_row[:, 0::2])\n",
        "    angle_rads_row[:, 1::2] = np.cos(angle_rads_row[:, 1::2])\n",
        "    angle_rads_col[:, 0::2] = np.sin(angle_rads_col[:, 0::2])\n",
        "    angle_rads_col[:, 1::2] = np.cos(angle_rads_col[:, 1::2])\n",
        "    pos_encoding = np.concatenate([angle_rads_row,angle_rads_col],axis=1)[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=TARGET_DTYPE)\n",
        "\n",
        "def positional_encoding_2d(row,col,d_model):\n",
        "    \n",
        "    row_pos = tf.repeat(tf.range(row), col)[:, tf.newaxis]\n",
        "    col_pos = tf.reshape(tf.repeat(tf.expand_dims(tf.range(col),0), row, axis=0), (-1, 1))\n",
        "\n",
        "    angle_rads_row = get_angles(tf.cast(row_pos, tf.float32), tf.range(d_model//2)[tf.newaxis,:], d_model//2)\n",
        "    angle_rads_col = get_angles(tf.cast(col_pos, tf.float32), tf.range(d_model//2)[tf.newaxis,:], d_model//2)\n",
        "\n",
        "    sin_angle_rads_row = tf.math.sin(angle_rads_row[:, ::2])\n",
        "    cos_angle_rads_row = tf.math.cos(angle_rads_row[:, 1::2])\n",
        "    angle_rads_row = do_interleave(sin_angle_rads_row, cos_angle_rads_row)\n",
        "\n",
        "    sin_angle_rads_col = tf.math.sin(angle_rads_col[:, ::2])\n",
        "    cos_angle_rads_col = tf.math.cos(angle_rads_col[:, 1::2])\n",
        "    angle_rads_col = do_interleave(sin_angle_rads_col, cos_angle_rads_col)\n",
        "    \n",
        "    pos_encoding = tf.concat([angle_rads_row,angle_rads_col],axis=1)[tf.newaxis, ...]\n",
        "    return pos_encoding\n",
        "\n",
        "pos_encoding = positional_encoding_1d(256, 512)\n",
        "\n",
        "print(pos_encoding.shape)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.pcolormesh(tf.cast(pos_encoding[0], tf.float32), cmap='RdBu')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylim((0, 256))\n",
        "plt.xlabel('Depth', fontweight=\"bold\")\n",
        "plt.ylabel('Position', fontweight=\"bold\")\n",
        "plt.title(\"Visualization of Positional Encoding\", fontweight=\"bold\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:19.581062Z",
          "iopub.execute_input": "2021-06-15T17:06:19.581484Z",
          "iopub.status.idle": "2021-06-15T17:06:19.603674Z",
          "shell.execute_reply.started": "2021-06-15T17:06:19.58145Z",
          "shell.execute_reply": "2021-06-15T17:06:19.602472Z"
        },
        "trusted": true,
        "id": "GY4jwmuAJsZx"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), TARGET_DTYPE)\n",
        "\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return tf.cast(mask, TARGET_DTYPE)\n",
        "\n",
        "def create_mask(inp, tar):\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    return tf.cast(combined_mask, TARGET_DTYPE)\n",
        "\n",
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "print(f\"x --\\n{x}\\n\\n\\nPADDING MASK\\n{create_padding_mask(x)}\\n\")\n",
        "print(\"\")\n",
        "\n",
        "x = tf.random.uniform((1, 5))\n",
        "print(f\"x.shape[1] -- {x.shape[1]}\")\n",
        "print(f\"\\n\\nLOOK-AHEAD MASK\\n{create_look_ahead_mask(x.shape[1])}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:19.606628Z",
          "iopub.execute_input": "2021-06-15T17:06:19.607303Z",
          "iopub.status.idle": "2021-06-15T17:06:19.659543Z",
          "shell.execute_reply.started": "2021-06-15T17:06:19.607249Z",
          "shell.execute_reply": "2021-06-15T17:06:19.657805Z"
        },
        "trusted": true,
        "id": "w05QmfGzJsZy"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  \n",
        "\n",
        "    \n",
        "    dk = tf.cast(tf.shape(k)[-1], TARGET_DTYPE)\n",
        "\n",
        "    \n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  \n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "\n",
        "def print_out(q, k, v):\n",
        "    temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n",
        "\n",
        "    print(f'Attention weights are:\\n\\t-->{temp_attn}')\n",
        "\n",
        "    print(f'\\nOutput is:\\n\\t-->{temp_out}')\n",
        "\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "\n",
        "temp_k = tf.constant([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype=TARGET_DTYPE)  # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[   1,0],\n",
        "                      [  10,0],\n",
        "                      [ 100,5],\n",
        "                      [1000,6]], dtype=TARGET_DTYPE)  # (4, 2)\n",
        "\n",
        "print(f\"\\n-----------------------\\n\\nTEMP K:\\n\\n{temp_k}\\n\")\n",
        "print(f\"\\n-----------------------\\n\\nTEMP V:\\n\\n{temp_v}\\n\")\n",
        "\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=TARGET_DTYPE)  # (1, 3)\n",
        "print(f\"\\n-----------------------\\n\\nTEMP Q:\\n\\n{temp_q} \\n\")\n",
        "print_out(temp_q, temp_k, temp_v)\n",
        "\n",
        "temp_q = tf.constant([[0, 0, 10]], dtype=TARGET_DTYPE)  # (1, 3)\n",
        "print(f\"\\n-----------------------\\n\\nTEMP Q:\\n\\n{temp_q} \\n\")\n",
        "print_out(temp_q, temp_k, temp_v)\n",
        "\n",
        "temp_q = tf.constant([[10, 10, 0]], dtype=TARGET_DTYPE)  # (1, 3)\n",
        "print(f\"\\n-----------------------\\n\\nTEMP Q:\\n\\n{temp_q} \\n\")\n",
        "print_out(temp_q, temp_k, temp_v)\n",
        "\n",
        "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=TARGET_DTYPE)  # (3, 3)\n",
        "print(f\"\\n-----------------------\\n\\nTEMP Q:\\n\\n{temp_q} \\n\")\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:19.661442Z",
          "iopub.execute_input": "2021-06-15T17:06:19.661932Z",
          "iopub.status.idle": "2021-06-15T17:06:19.845768Z",
          "shell.execute_reply.started": "2021-06-15T17:06:19.66188Z",
          "shell.execute_reply": "2021-06-15T17:06:19.844578Z"
        },
        "trusted": true,
        "id": "C8RHsvjGJsZy"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        assert d_model % self.num_heads == 0\n",
        "        \n",
        "        self.depth = d_model // self.num_heads\n",
        "        \n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "    \n",
        "        q = self.wq(q)  \n",
        "    \n",
        "        k = self.wk(k)  \n",
        "    \n",
        "        v = self.wv(v)  \n",
        "\n",
        "    \n",
        "        q = self.split_heads(q, batch_size)  \n",
        "    \n",
        "        k = self.split_heads(k, batch_size)  \n",
        "    \n",
        "        v = self.split_heads(v, batch_size)  \n",
        "        \n",
        "        scaled_attention, attention_weights = \\\n",
        "            scaled_dot_product_attention(q, k, v, mask)\n",
        "    \n",
        "        \n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
        "\n",
        "        \n",
        "        concat_attention = \\\n",
        "            tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  \n",
        "\n",
        "        \n",
        "        output = self.dense(concat_attention)  \n",
        "            \n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512), dtype=TARGET_DTYPE)  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, y, y, mask=None)\n",
        "print(f\"Custom MHA Layer:\\n\\t-->{out[0,:2]}\\n\\t-->{(out.shape, attn.shape)}\\n\")\n",
        "\n",
        "# TF NATIVE\n",
        "temp_mha = tf.keras.layers.MultiHeadAttention(8, 512)\n",
        "out, attn = temp_mha(y, y, y, attention_mask=None, return_attention_scores=True)\n",
        "print(f\"TF MHA Layer:\\n\\t-->{out[0,:2]}\\n\\t-->{(out.shape, attn.shape)}\\n\")\n",
        "\n",
        "del temp_mha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUJXk8qxJsZy"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 16px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.2.5 TRANSFORMER - POINT-WISE FEED FORWARD NEURAL NETWORK</h3>\n",
        "\n",
        "---\n",
        "\n",
        "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in-between."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:19.848205Z",
          "iopub.execute_input": "2021-06-15T17:06:19.84855Z",
          "iopub.status.idle": "2021-06-15T17:06:19.923279Z",
          "shell.execute_reply.started": "2021-06-15T17:06:19.848518Z",
          "shell.execute_reply": "2021-06-15T17:06:19.9222Z"
        },
        "trusted": true,
        "id": "I0Db1YHNJsZy"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        \n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  \n",
        "        \n",
        "        tf.keras.layers.Dense(d_model)  \n",
        "    ])\n",
        "\n",
        "sample_ffn = point_wise_feed_forward_network(512, D_FF)\n",
        "print(\"\\nFFN INPUT & OUTPUT SHAPE: \" \\\n",
        "      f\"{sample_ffn(tf.random.uniform((64, 50, 512), dtype=TARGET_DTYPE)).shape}\" \\\n",
        "      \"\\n\\nFFN SUMMARY:\")\n",
        "print(sample_ffn.summary())\n",
        "\n",
        "del sample_ffn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:19.924783Z",
          "iopub.execute_input": "2021-06-15T17:06:19.92511Z",
          "iopub.status.idle": "2021-06-15T17:06:20.083725Z",
          "shell.execute_reply.started": "2021-06-15T17:06:19.925079Z",
          "shell.execute_reply": "2021-06-15T17:06:20.082845Z"
        },
        "trusted": true,
        "id": "JkRC0HksJsZz"
      },
      "source": [
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        \n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads, key_dim=d_model,)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "    def call(self, x, training, mask=None):\n",
        "        \n",
        "        attn_output, _ = self.mha(x, x, x, mask, return_attention_scores=True) \n",
        "\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        \n",
        "        out1 = self.layernorm1(x + attn_output, training=training)  \n",
        "        \n",
        "        ffn_output = self.ffn(out1, training=training)  \n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        \n",
        "        out2 = self.layernorm2(out1 + ffn_output, training=training)  \n",
        "        \n",
        "        return out2\n",
        "\n",
        "sample_encoder_layer = TransformerEncoderLayer(D_MODEL, 8, D_FF)\n",
        "sample_encoder_layer_output = sample_encoder_layer(demo_encoder_output, training=False, mask=None)\n",
        "del sample_encoder_layer\n",
        "\n",
        "# (batch_size, input_seq_len, d_model)\n",
        "sample_encoder_layer_output.shape  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:20.085177Z",
          "iopub.execute_input": "2021-06-15T17:06:20.085526Z",
          "iopub.status.idle": "2021-06-15T17:06:20.291921Z",
          "shell.execute_reply.started": "2021-06-15T17:06:20.085497Z",
          "shell.execute_reply": "2021-06-15T17:06:20.290896Z"
        },
        "trusted": true,
        "id": "p5-VOqDJJsZz"
      },
      "source": [
        "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "\n",
        "        # WE COULD USE A CUSTOM DEFINED MHA MODEL BUT WE WILL USE TFA INSTEAD\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        \n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "    \n",
        "        # Layer Normalization Layers\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Dropout Layers\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "        \n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "\n",
        "        out1 = self.layernorm1(attn1 + x, training=training)\n",
        "    \n",
        "       \n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask) \n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        \n",
        "        # Residual connection followed by layer normalization\n",
        "        #   – (batch_size, target_seq_len, d_model)\n",
        "        out2 = self.layernorm2(attn2 + out1, training=training)  \n",
        "        \n",
        "        # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.ffn(out2, training=training)  \n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "\n",
        "        # Residual connection followed by layer normalization\n",
        "        #   – (batch_size, target_seq_len, d_model)\n",
        "        out3 = self.layernorm3(ffn_output + out2, training=training)  \n",
        "        \n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "sample_decoder_layer = TransformerDecoderLayer(D_MODEL, 8, D_FF)\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(tf.random.uniform((BATCH_SIZE_DEBUG, MAX_LEN, D_MODEL), dtype=TARGET_DTYPE), sample_encoder_layer_output, False, None, None)\n",
        "del sample_decoder_layer\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:20.293269Z",
          "iopub.execute_input": "2021-06-15T17:06:20.293578Z",
          "iopub.status.idle": "2021-06-15T17:06:20.61255Z",
          "shell.execute_reply.started": "2021-06-15T17:06:20.293543Z",
          "shell.execute_reply": "2021-06-15T17:06:20.611245Z"
        },
        "trusted": true,
        "id": "GzFayxnoJsZz"
      },
      "source": [
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 maximum_position_encoding, dropout_rate=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.pos_encoding = positional_encoding_1d(maximum_position_encoding, self.d_model)\n",
        "        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        \n",
        "    def call(self, x, training, mask=None):\n",
        "        \n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        #   – (batch_size, input_seq_len, d_model)\n",
        "        x += self.pos_encoding\n",
        "        x = self.dropout(x, training=training)\n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        #   – (batch_size, input_seq_len, d_model)\n",
        "        return x  \n",
        "\n",
        "sample_encoder = TransformerEncoder(num_layers=2, \n",
        "                         d_model=D_MODEL, \n",
        "                         num_heads=8, \n",
        "                         dff=D_FF,\n",
        "                         maximum_position_encoding=IMG_SEQ_LEN)\n",
        "\n",
        "sample_encoder_output = sample_encoder(demo_encoder_output, training=False, mask=None)\n",
        "del sample_encoder\n",
        "\n",
        "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:20.613939Z",
          "iopub.execute_input": "2021-06-15T17:06:20.61425Z",
          "iopub.status.idle": "2021-06-15T17:06:21.025314Z",
          "shell.execute_reply.started": "2021-06-15T17:06:20.614221Z",
          "shell.execute_reply": "2021-06-15T17:06:21.024034Z"
        },
        "trusted": true,
        "id": "S9n-A1kWJsZ0"
      },
      "source": [
        "class TransformerDecoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding_1d(maximum_position_encoding, d_model)\n",
        "        \n",
        "        self.dec_layers = [TransformerDecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        \n",
        "        # adding embedding and position encoding.\n",
        "        #   – (batch_size, target_seq_len, d_model)\n",
        "        x = self.embedding(x)  \n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, TARGET_DTYPE))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        \n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n",
        "\n",
        "\n",
        "sample_decoder = TransformerDecoder(num_layers=2, d_model=D_MODEL, num_heads=8, \n",
        "                         dff=D_FF, target_vocab_size=VOCAB_LEN,\n",
        "                         maximum_position_encoding=MAX_LEN)\n",
        "temp_input = tf.random.uniform((1, MAX_LEN), dtype=tf.int64, minval=0, maxval=VOCAB_LEN)\n",
        "output, attn = sample_decoder(temp_input, \n",
        "                              enc_output=demo_encoder_output, \n",
        "                              training=False,\n",
        "                              look_ahead_mask=None, \n",
        "                              padding_mask=None)\n",
        "del sample_decoder\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:21.027065Z",
          "iopub.execute_input": "2021-06-15T17:06:21.027513Z",
          "iopub.status.idle": "2021-06-15T17:06:21.037777Z",
          "shell.execute_reply.started": "2021-06-15T17:06:21.027464Z",
          "shell.execute_reply": "2021-06-15T17:06:21.036534Z"
        },
        "trusted": true,
        "id": "kxkrE4RSJsZ0"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 target_vocab_size, pe_input, pe_target, dropout_rate=0.1):\n",
        "        \"\"\"TBD\"\"\"\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.t_encoder = TransformerEncoder(num_layers, d_model, num_heads, dff, pe_input, dropout_rate)\n",
        "        self.t_decoder = TransformerDecoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, dropout_rate)\n",
        "        self.t_final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, t_inp, t_tar, \n",
        "             training, enc_padding_mask=None, \n",
        "             look_ahead_mask=None, dec_padding_mask=None):\n",
        "        \"\"\"TBD\"\"\"\n",
        "        # (batch_size, inp_seq_len, d_model)\n",
        "        enc_output = self.t_encoder(t_inp, training, enc_padding_mask)  \n",
        "        \n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.t_decoder(t_tar, \n",
        "                                                       enc_output, \n",
        "                                                       training, \n",
        "                                                       look_ahead_mask, \n",
        "                                                       dec_padding_mask)\n",
        "\n",
        "        # (batch_size, tar_seq_len, target_vocab_size)\n",
        "        final_output = self.t_final_layer(dec_output)  \n",
        "    \n",
        "        return final_output, attention_weights\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGk22ehEJsZ0"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.3 CREATE A LEARNING RATE SCHEDULER</h3>\n",
        "\n",
        "---\n",
        "\n",
        "We utiliize the learning rate scheduler from the \"Attention Is All You Need\" paper with some minor tweaks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:21.039601Z",
          "iopub.execute_input": "2021-06-15T17:06:21.040314Z",
          "iopub.status.idle": "2021-06-15T17:06:21.220802Z",
          "shell.execute_reply.started": "2021-06-15T17:06:21.040259Z",
          "shell.execute_reply": "2021-06-15T17:06:21.219693Z"
        },
        "trusted": true,
        "id": "ufsmJE9aJsZ0"
      },
      "source": [
        "print(\"\\n... LEARNING RATE SCHEDULE CREATION STARTING ...\\n\")\n",
        "\n",
        "# Part of the Training Configuration\n",
        "EPOCHS = 30\n",
        "TOTAL_STEPS = TRAIN_STEPS*EPOCHS\n",
        "\n",
        "# Learning Rate Scheduler Configuration\n",
        "WARM_STEPS = (TRAIN_STEPS-1)*4 # Suuuuuper long ramp-up\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model*1.75) * tf.math.minimum(arg1, arg2)\n",
        "    \n",
        "temp_learning_rate_schedule = CustomSchedule(D_MODEL, WARM_STEPS)\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(TRAIN_STEPS*EPOCHS, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n... LEARNING RATE SCHEDULE CREATION FINISHED ...\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyL9OMC_JsZ2"
      },
      "source": [
        "# MODEL TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNuYJm5DJsZ2"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">6.1 INDIVIDUAL TRAIN STEP</h3>\n",
        "\n",
        "---\n",
        "\n",
        "INFORMATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:45.712428Z",
          "iopub.execute_input": "2021-06-15T17:06:45.712753Z",
          "iopub.status.idle": "2021-06-15T17:06:46.385415Z",
          "shell.execute_reply.started": "2021-06-15T17:06:45.712721Z",
          "shell.execute_reply": "2021-06-15T17:06:46.384331Z"
        },
        "trusted": true,
        "id": "JSojYZT4JsZ2"
      },
      "source": [
        "def train_step(_image_batch, _inchi_batch):\n",
        "    \"\"\" Forward pass (calculate gradients)\n",
        "    \n",
        "    Args:\n",
        "        _image_batch (): TBD\n",
        "        _inchi_batch (): TBD\n",
        "    \n",
        "    Returns:\n",
        "        tbd\n",
        "    \"\"\"\n",
        "    _inchi_batch_input  = _inchi_batch[:, :-1]\n",
        "    _inchi_batch_target = _inchi_batch[:, 1:]\n",
        "    combined_mask = create_mask(_inchi_batch_input, _inchi_batch_target)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        _image_embedding = encoder(_image_batch, training=True)\n",
        "        prediction_batch, _ = transformer(_image_embedding, _inchi_batch_input, training=True, look_ahead_mask=combined_mask)\n",
        "        \n",
        "        # Update Loss Accumulator\n",
        "        batch_loss = loss_fn(_inchi_batch_target, prediction_batch)/(MAX_LEN-1)\n",
        "\n",
        "        # Update Accuracy Metric\n",
        "        metrics[\"train_acc\"].update_state(_inchi_batch_target, prediction_batch, \n",
        "                                          sample_weight=tf.where(tf.not_equal(_inchi_batch_target, PAD_TOKEN), 1.0, 0.0))\n",
        "\n",
        "\n",
        "    # backpropagation using variables, gradients and loss\n",
        "    #    - split this into two seperate optimizers/lrs/etc in the future\n",
        "    #    - we use the batch loss accumulation to update gradients\n",
        "    gradients = tape.gradient(batch_loss, encoder.trainable_variables + transformer.trainable_variables)\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, 10.0)\n",
        "    optimizer.apply_gradients(zip(gradients, encoder.trainable_variables+transformer.trainable_variables))\n",
        "    \n",
        "    metrics[\"batch_loss\"].update_state(batch_loss)\n",
        "    metrics[\"train_loss\"].update_state(batch_loss)\n",
        "\n",
        "@tf.function\n",
        "def dist_train_step(_image_batch, _inchi_batch):\n",
        "    strategy.run(train_step, args=(_image_batch, _inchi_batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7--fEr5oJsZ2"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">6.2 INDIVIDUAL VAL STEP</h3>\n",
        "\n",
        "---\n",
        "\n",
        "INFORMATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:06:46.388534Z",
          "iopub.execute_input": "2021-06-15T17:06:46.389001Z",
          "iopub.status.idle": "2021-06-15T17:06:46.40076Z",
          "shell.execute_reply.started": "2021-06-15T17:06:46.388936Z",
          "shell.execute_reply": "2021-06-15T17:06:46.399625Z"
        },
        "trusted": true,
        "id": "BuPOVTZOJsZ3"
      },
      "source": [
        "def val_step(_image_batch, _inchi_batch):\n",
        "    \"\"\" Forward pass (calculate gradients)\n",
        "    \n",
        "    Args:\n",
        "        image_batch (): TBD\n",
        "        inchi_batch (): TBD\n",
        "    \n",
        "    Returns:\n",
        "        tbd\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize batch_loss\n",
        "    batch_loss = tf.constant(0.0, TARGET_DTYPE)       \n",
        "    transformer_pred_batch = tf.ones((REPLICA_BATCH_SIZE, 1), dtype=tf.uint8)\n",
        "    \n",
        "    # Get image embedding (once)\n",
        "    _image_embedding = encoder(_image_batch, training=False)\n",
        "    \n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for c_idx in range(1, MAX_LEN):\n",
        "        gt_batch_id = _inchi_batch[:, c_idx]\n",
        "        combined_mask = create_mask(_inchi_batch, transformer_pred_batch)\n",
        "        \n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        prediction_batch, attention_weights = transformer(_image_embedding, transformer_pred_batch, training=False, look_ahead_mask=combined_mask)\n",
        "        predicted_batch_id = prediction_batch[:, -1:, :]\n",
        "        \n",
        "        # Update Loss Accumulator\n",
        "        batch_loss += loss_fn(gt_batch_id, predicted_batch_id[:, -1])\n",
        "    \n",
        "        # Update Accuracy Metric\n",
        "        metrics[\"val_acc\"].update_state(gt_batch_id, predicted_batch_id[:, -1],\n",
        "                                        sample_weight=tf.where(tf.not_equal(gt_batch_id, PAD_TOKEN), 1.0, 0.0))\n",
        "\n",
        "        # no teacher forcing, predicted char is next transformer input\n",
        "        transformer_pred_batch = tf.concat([transformer_pred_batch, tf.cast(tf.argmax(predicted_batch_id, axis=-1), tf.uint8)], axis=-1)\n",
        "        \n",
        "    # Update Loss Metric\n",
        "    metrics[\"val_loss\"].update_state(batch_loss)\n",
        "    return transformer_pred_batch    \n",
        "\n",
        "    \n",
        "@tf.function\n",
        "def dist_val_step(_val_dist_ds):\n",
        "    _val_image_batch, _val_inchi_batch = next(_val_dist_ds)\n",
        "    predictions_seq_batch_per_replica = strategy.run(val_step, args=(_val_image_batch, _val_inchi_batch))\n",
        "    predictions_seq_batch_accum = strategy.gather(predictions_seq_batch_per_replica, axis=0)\n",
        "    _val_inchi_batch_accum = strategy.gather(_val_inchi_batch, axis=0)\n",
        "    return predictions_seq_batch_accum, _val_inchi_batch_accum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TjlG49SJsZ3"
      },
      "source": [
        "#INFER ON TEST DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUO7tS5mJsZ3"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">7.1 INDIVIDUAL TEST STEP (AND DISTRIBUTED)</h3>\n",
        "\n",
        "---\n",
        "\n",
        "INFORMATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9EnzdIyxJsZ4"
      },
      "source": [
        "def test_step(_image_batch):\n",
        "    \n",
        "    \n",
        "    transformer_pred_batch = tf.ones((REPLICA_BATCH_SIZE, 1), dtype=tf.uint8)\n",
        "    \n",
        "    # Get image embedding (once)\n",
        "    _image_embedding = encoder(_image_batch, training=False)\n",
        "    \n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for c_idx in range(1, MAX_LEN):\n",
        "        \n",
        "        combined_mask = create_mask(None, transformer_pred_batch)\n",
        "        \n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        prediction_batch, attention_weights = transformer(_image_embedding, transformer_pred_batch, training=False, look_ahead_mask=combined_mask)\n",
        "        predicted_batch_id = prediction_batch[:, -1:, :]\n",
        "        \n",
        "        # no teacher forcing, predicted char is next transformer input\n",
        "        transformer_pred_batch = tf.concat([transformer_pred_batch, tf.cast(tf.argmax(predicted_batch_id, axis=-1), tf.uint8)], axis=-1)\n",
        "        \n",
        "    return transformer_pred_batch \n",
        "\n",
        "    \n",
        "@tf.function\n",
        "def distributed_test_step(_img_batch, _img_ids):\n",
        "    per_replica_seqs = strategy.run(test_step, args=(_img_batch,))\n",
        "    predictions = strategy.gather(per_replica_seqs, axis=0)\n",
        "    pred_ids = strategy.gather(_img_ids, axis=0)\n",
        "    return predictions, pred_ids\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def distributed_test_step_v2(_test_dist_ds):\n",
        "    _test_image_batch, _test_id_batch = next(_test_dist_ds)\n",
        "    predictions_seq_batch_per_replica = strategy.run(test_step, args=(_test_image_batch,))\n",
        "    predictions_seq_batch_accum = strategy.gather(predictions_seq_batch_per_replica, axis=0)\n",
        "    _test_id_batch_accum = strategy.gather(_test_id_batch, axis=0)\n",
        "    return predictions_seq_batch_accum, _test_id_batch_accum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIiJZD_iJsZ4"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">7.2 RAW INFERENCE LOOP</h3>\n",
        "\n",
        "---\n",
        "\n",
        "INFORMATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JE0hGD5jJsZ4"
      },
      "source": [
        "# To Store The Preds\n",
        "all_pred_arr = tf.zeros((1, MAX_LEN), dtype=tf.uint8)\n",
        "all_pred_ids = tf.zeros((1, 1), dtype=tf.string)\n",
        "\n",
        "# Create an iterator\n",
        "dist_test_ds = iter(strategy.experimental_distribute_dataset(test_ds))\n",
        "for i in tqdm(range(TEST_STEPS), total=TEST_STEPS): \n",
        "    preds, pred_ids = distributed_test_step_v2(dist_test_ds)\n",
        "    all_pred_arr = tf.concat([all_pred_arr, preds], axis=0)\n",
        "    all_pred_ids = tf.concat([all_pred_ids, tf.expand_dims(pred_ids, axis=-1)], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MkErboWWJsZ4"
      },
      "source": [
        "# To Store The Preds\n",
        "all_pred_arr = tf.zeros((1, MAX_LEN), dtype=tf.uint8)\n",
        "all_pred_ids = tf.zeros((1, 1), dtype=tf.string)\n",
        "\n",
        "# Create an iterator\n",
        "dist_test_ds = iter(strategy.experimental_distribute_dataset(test_ds))\n",
        "for i in tqdm(range(TEST_STEPS), total=TEST_STEPS): \n",
        "    img_batch, id_batch = next(dist_test_ds)\n",
        "    preds, pred_ids = distributed_test_step(img_batch, id_batch)\n",
        "    all_pred_arr = tf.concat([all_pred_arr, preds], axis=0)\n",
        "    all_pred_ids = tf.concat([all_pred_ids, tf.expand_dims(pred_ids, axis=-1)], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0SwxlNHJsZ4"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">7.3 TEST PRED POST-PROCESSING</h3>\n",
        "\n",
        "---\n",
        "\n",
        "INFORMATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qq3pEyU7JsZ4"
      },
      "source": [
        "def arr_2_inchi(arr):\n",
        "    inchi_str = ''\n",
        "    for i in arr:\n",
        "        c = int_2_tok.get(i)\n",
        "        if c==\"<END>\":\n",
        "            break\n",
        "        inchi_str += c\n",
        "    return inchi_str\n",
        "\n",
        "pred_df = pd.DataFrame({\n",
        "    \"image_id\":[x[0].decode() for x in tqdm(all_pred_ids[1:-REQUIRED_DATASET_PAD].numpy(), total=N_TEST)], \n",
        "    \"InChI\":[arr_2_inchi(pred_arr) for pred_arr in tqdm(all_pred_arr[1:-REQUIRED_DATASET_PAD].numpy(), total=N_TEST)]\n",
        "})\n",
        "\n",
        "pred_df = pred_df.sort_values(by=\"image_id\").reset_index(drop=True)\n",
        "pred_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbrqi2CFJsZ4"
      },
      "source": [
        "<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">7.4 SAVE SUBMISSION.CSV</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "E661pEeXJsZ4"
      },
      "source": [
        "pred_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}